{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural-Fly: Domain Adversarially Invariant Meta Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script loads quadrotor flight data in different wind conditions, trains a wind invariant representation of the unmodeled aerodynamics, and tests the performance of the model when adapting to new data in different wind conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import utils\n",
    "import mlmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on win32, setting 0 workers\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    NUM_WORKERS = 0 # Windows does not support multiprocessing\n",
    "else:\n",
    "    NUM_WORKERS = 2\n",
    "print('running on ' + sys.platform + ', setting ' + str(NUM_WORKERS) + ' workers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create some simple visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural-fly_dim-a-4_v-q-pwm\n"
     ]
    }
   ],
   "source": [
    "dim_a = 4\n",
    "features = ['v', 'q', 'pwm']\n",
    "label = 'fa'\n",
    "\n",
    "# Training data collected from the neural-fly drone\n",
    "dataset = 'neural-fly' \n",
    "dataset_folder = 'data/training'\n",
    "hover_pwm_ratio = 1.\n",
    "\n",
    "# # Training data collected from an intel aero drone\n",
    "# dataset = 'neural-fly-transfer'\n",
    "# dataset_folder = 'data/training-transfer'\n",
    "# hover_pwm = 910 # mean hover pwm for neural-fly drone\n",
    "# intel_hover_pwm = 1675 # mean hover pwm for intel-aero drone\n",
    "# hover_pwm_ratio = hover_pwm / intel_hover_pwm # scaling ratio from system id\n",
    "\n",
    "modelname = f\"{dataset}_dim-a-{dim_a}_{'-'.join(features)}\" # 'intel-aero_fa-num-Tsp_v-q-pwm'\n",
    "print(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45339479 -0.08809626 -0.8517047  -0.01542406 -0.11135551 -0.00327016\n",
      " -0.99365535  0.87293205  0.93758716  0.89263749  0.95050831]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "RawData = utils.load_data(dataset_folder)\n",
    "Data = utils.format_data(RawData, features=features, output=label)\n",
    "print(Data[0][0][0])\n",
    "testdata_folder = 'data/experiment'\n",
    "RawData = utils.load_data(testdata_folder, expnames='(baseline_)([0-9]*|no)wind')\n",
    "TestData = utils.format_data(RawData, features=features, output=label, hover_pwm_ratio=hover_pwm_ratio) # wind condition label, C, will not make sense for this data - that's okay since C is only used in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in Data:\n",
    "    utils.plot_subdataset(data, features, title_prefix=\"(Training data)\")\n",
    "    break\n",
    "\n",
    "for data in TestData:\n",
    "    utils.plot_subdataset(data, features, title_prefix=\"(Testing Data)\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {}\n",
    "options['dim_x'] = Data[0].X.shape[1]\n",
    "options['dim_y'] = Data[0].Y.shape[1]\n",
    "options['num_c'] = len(Data)\n",
    "print(len(Data))\n",
    "print('dims of (x, y) are', (options['dim_x'], options['dim_y']))\n",
    "print('there are ' + str(options['num_c']) + ' different conditions')\n",
    "print(Data[0].Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "options['features'] = features\n",
    "options['dim_a'] = dim_a\n",
    "options['loss_type'] = 'crossentropy-loss'\n",
    "\n",
    "options['shuffle'] = True # True: shuffle trajectories to data points\n",
    "options['K_shot'] = 32 # number of K-shot for least square on a\n",
    "options['phi_shot'] = 256 # batch size for training phi\n",
    "\n",
    "options['alpha'] = 0.01 # adversarial regularization loss\n",
    "options['learning_rate'] = 5e-4\n",
    "options['frequency_h'] = 2 # how many times phi is updated between h updates, on average\n",
    "options['SN'] = 2. # maximum single layer spectral norm of phi\n",
    "options['gamma'] = 10. # max 2-norm of a\n",
    "options['num_epochs'] = 200\n",
    "\n",
    "print(features)\n",
    "print(dim_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adaptation dataset will be used to update $a$ in each training loop.\n",
    "The training dataset will be used to train $\\phi$ in each training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainset = []\n",
    "# Adaptset = []\n",
    "Trainloader = []\n",
    "Adaptloader = []\n",
    "for i in range(options['num_c']):\n",
    "    fullset = mlmodel.MyDataset(Data[i].X, Data[i].Y, Data[i].C)\n",
    "    \n",
    "    l = len(Data[i].X)\n",
    "    if options['shuffle']:\n",
    "        trainset, adaptset = random_split(fullset, [int(2/3*l), l-int(2/3*l)])\n",
    "    else:\n",
    "        trainset = mlmodel.MyDataset(Data[i].X[:int(2/3*l)], Data[i].Y[:int(2/3*l)], Data[i].C) \n",
    "        adaptset = mlmodel.MyDataset(Data[i].X[int(2/3*l):], Data[i].Y[int(2/3*l):], Data[i].C)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=options['phi_shot'], shuffle=options['shuffle'], num_workers=NUM_WORKERS)\n",
    "    adaptloader = torch.utils.data.DataLoader(adaptset, batch_size=options['K_shot'], shuffle=options['shuffle'], num_workers=NUM_WORKERS)\n",
    "   \n",
    "    # Trainset.append(trainset)\n",
    "    # Adaptset.append(adaptset)\n",
    "    Trainloader.append(trainloader) # for training phi\n",
    "    Adaptloader.append(adaptloader) # for LS on a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adversarially Invariant Meta Learning\n",
    "\n",
    "Assume the state $x\\in\\mathbb{R}^n$ and $c$ is hidden state used to represent changing environment. We are interested in learning some function $f(x(t),c(t))$. $f(x(t),c(t))$ can be separated into three terms: $$f(x(t),c(t))=\\phi(x(t))a(c(t))+d(t),$$\n",
    "where $\\phi(x(t))$ captures the $c$-variant part and $a(c(t))\\in\\mathbb{R}^m$ is implicitly a function of the hidden state $c(t)$. Finally, $d(t)$ is the residual noise term.\n",
    "\n",
    "We want to learn $\\phi(x)$ such that it doesn't include any information about $c$. To reach this goal, we introduce another neural network $h$ where $h(\\phi(x))$ tries to predict $c$.\n",
    "\n",
    "The loss function is given as\n",
    "$$\\max_h\\min_{\\phi, \\left\\{a_{c_j}\\right\\}_j}\\sum_{j}\\sum_{i}\\left\\|\\phi(x^{(i)}_{c_j})a_{c_j}-f(x^{(i)}_{c_j},c_j)\\right\\|^2-\\alpha\\cdot\\text{CrossEntropy}\\left(h(\\phi(x^{(i)}_{c_j})),j\\right)$$\n",
    "Note that the $\\text{CrossEntropy-loss}$ will not require physical encoding of $c_j$ in training, only a label for $c$ that corresponds to the subdataset (that is, the label $c$ has no physical meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model class definition in an external file so they can be referenced outside this script\n",
    "phi_net = mlmodel.Phi_Net(options)\n",
    "h_net = mlmodel.H_Net_CrossEntropy(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "criterion_h = nn.CrossEntropyLoss()\n",
    "optimizer_h = optim.Adam(h_net.parameters(), lr=options['learning_rate'])\n",
    "optimizer_phi = optim.Adam(phi_net.parameters(), lr=options['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Training Algorithm\n",
    "\n",
    "**Step 0: sample $c$, and sample $B+K$ data points in correponding subdataset $\\{x_i,c,f(x_i,c)\\}_{i}$**\n",
    "\n",
    "**Step 1: estimate $a$ using least-square**\n",
    "\n",
    "$K$ data points (sampled from the same wind condition $c$) are used to compute $a$ using least-squares, i.e., adaptation:\n",
    "$$\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        \\phi(x_1) \\\\ \\phi(x_2) \\\\ \\vdots \\\\ \\phi(x_K) \n",
    "    \\end{bmatrix}}\n",
    "    _{\\Phi\\in\\mathbb{R}^{K\\times \\dim(a)}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        a_1 & \\cdots & a_{\\dim(y)} \n",
    "    \\end{bmatrix}}\n",
    "    _{a\\in\\mathbb{R}^{\\dim(a)\\times \\dim(y)}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        f_1(x_1) & \\cdots & f_{\\dim(y)}(x_1) \\\\ f_1(x_2) & \\cdots & f_{\\dim(y)}(x_2) \\\\ \\vdots & \\vdots & \\vdots\\\\ f_1(x_K) & \\cdots & f_{\\dim(y)}(x_K) \n",
    "    \\end{bmatrix}}\n",
    "    _{Y\\in\\mathbb{R}^{K\\times \\dim(y)}}\n",
    "$$\n",
    "\n",
    "The least square solution is given by\n",
    "$$a=(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top Y$$\n",
    "Normalization on $a$ is implemented to avoid ambiguity of $\\phi(x)a$ (since $\\phi(x)a=(0.1\\phi(x))\\cdot(10a)$):\n",
    "$$a\\leftarrow \\gamma\\cdot\\frac{a}{\\|a\\|_F},\\quad\\text{if}\\,\\,\\|a\\|_F>\\gamma$$\n",
    "Note that $a$ is an implicit function of $\\phi$.\n",
    "\n",
    "**Step 2: fix $h$ and train $\\phi$**\n",
    "\n",
    "With this $a$, another $B$ data points (with same $c$) are used for gradient descent with loss\n",
    "$$\\mathcal{L}(\\phi)=\\|f(x)-\\phi(x)a\\|_2^2-\\alpha\\cdot\\|h(\\phi(x))-c\\|_2^2$$\n",
    "\n",
    "**Step 3: fix $\\phi$ and train discriminator $h$**\n",
    "\n",
    "Finally, these $B$ data points are used again for gradient descent on $h$ with loss\n",
    "$$\\mathcal{L}(h)=\\|h(\\phi(x))-c\\|_2^2$$\n",
    "We may run this step less frequently than step 2, to improve stability in training (a trick from GAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save_freq = 200 # How often to save the model\n",
    "\n",
    "# Create some arrays to save training statistics\n",
    "Loss_f = [] # combined force prediction loss\n",
    "Loss_c = [] # combined adversarial loss\n",
    "\n",
    "# Loss for each subdataset \n",
    "Loss_test_nominal = [] # loss without any learning\n",
    "Loss_test_mean = [] # loss with mean predictor\n",
    "Loss_test_phi = [] # loss with NN\n",
    "for i in range(len(TestData)):\n",
    "    Loss_test_nominal.append([])\n",
    "    Loss_test_mean.append([])\n",
    "    Loss_test_phi.append([])\n",
    "\n",
    "# Training!\n",
    "for epoch in range(options['num_epochs']):\n",
    "    # Randomize the order in which we train over the subdatasets\n",
    "    arr = np.arange(options['num_c'])\n",
    "    np.random.shuffle(arr)\n",
    "\n",
    "    # Running loss over all subdatasets\n",
    "    running_loss_f = 0.0\n",
    "    running_loss_c = 0.0\n",
    "\n",
    "    for i in arr:\n",
    "        with torch.no_grad():\n",
    "            adaptloader = Adaptloader[i]\n",
    "            kshot_data = next(iter(adaptloader))\n",
    "            trainloader = Trainloader[i]\n",
    "            data = next(iter(trainloader))\n",
    "        \n",
    "        optimizer_phi.zero_grad()\n",
    "        \n",
    "        '''\n",
    "        Least-square to get $a$ from K-shot data\n",
    "        '''\n",
    "        X = kshot_data['input'] # K x dim_x\n",
    "        Y = kshot_data['output'] # K x dim_y\n",
    "        Phi = phi_net(X) # K x dim_a\n",
    "        Phi_T = Phi.transpose(0, 1) # dim_a x K\n",
    "        A = torch.inverse(torch.mm(Phi_T, Phi)) # dim_a x dim_a\n",
    "        a = torch.mm(torch.mm(A, Phi_T), Y) # dim_a x dim_y\n",
    "        if torch.norm(a, 'fro') > options['gamma']:\n",
    "            a = a / torch.norm(a, 'fro') * options['gamma']\n",
    "            \n",
    "        '''\n",
    "        Batch training \\phi_net\n",
    "        '''\n",
    "        inputs = data['input'] # B x dim_x\n",
    "        labels = data['output'] # B x dim_y\n",
    "        \n",
    "        c_labels = data['c'].type(torch.long)\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = torch.mm(phi_net(inputs), a)\n",
    "        loss_f = criterion(outputs, labels)\n",
    "        temp = phi_net(inputs)\n",
    "        \n",
    "        loss_c = criterion_h(h_net(temp), c_labels)\n",
    "            \n",
    "        loss_phi = loss_f - options['alpha'] * loss_c\n",
    "        loss_phi.backward()\n",
    "        optimizer_phi.step()\n",
    "        \n",
    "        '''\n",
    "        Discriminator training\n",
    "        '''\n",
    "        if np.random.rand() <= 1.0 / options['frequency_h']:\n",
    "            optimizer_h.zero_grad()\n",
    "            temp = phi_net(inputs)\n",
    "            \n",
    "            loss_c = criterion_h(h_net(temp), c_labels)\n",
    "            \n",
    "            loss_h = loss_c\n",
    "            loss_h.backward()\n",
    "            optimizer_h.step()\n",
    "        \n",
    "        '''\n",
    "        Spectral normalization\n",
    "        '''\n",
    "        if options['SN'] > 0:\n",
    "            for param in phi_net.parameters():\n",
    "                M = param.detach().numpy()\n",
    "                if M.ndim > 1:\n",
    "                    s = np.linalg.norm(M, 2)\n",
    "                    if s > options['SN']:\n",
    "                        param.data = param / s * options['SN']\n",
    "         \n",
    "        running_loss_f += loss_f.item()\n",
    "        running_loss_c += loss_c.item()\n",
    "    \n",
    "    # Save statistics\n",
    "    Loss_f.append(running_loss_f / options['num_c'])\n",
    "    Loss_c.append(running_loss_c / options['num_c'])\n",
    "    if epoch % 10 == 0:\n",
    "        print('[%d] loss_f: %.2f loss_c: %.2f' % (epoch, running_loss_f / options['num_c'], running_loss_c / options['num_c']))\n",
    "\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for j in range(len(TestData)):\n",
    "            loss_nominal, loss_mean, loss_phi = mlmodel.error_statistics(TestData[j].X, TestData[j].Y, phi_net, h_net, options=options)\n",
    "            Loss_test_nominal[j].append(loss_nominal)\n",
    "            Loss_test_mean[j].append(loss_mean)\n",
    "            Loss_test_phi[j].append(loss_phi)\n",
    "\n",
    "    # if epoch % model_save_freq == 0:\n",
    "    #     mlmodel.save_model(phi_net=phi_net, h_net=h_net, modelname=modelname + '-epoch-' + str(epoch), options=options)\n",
    "\n",
    "mlmodel.save_model(phi_net=phi_net, h_net=h_net, modelname=modelname + '-epoch-' + str(epoch), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(Loss_f)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('f-loss [N]')\n",
    "plt.title('training f loss')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(Loss_c)\n",
    "plt.title('training c loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('c-loss')\n",
    "plt.tight_layout()\n",
    "\n",
    "# for j in range(len(TestData)):\n",
    "#     plt.figure()\n",
    "#     # plt.plot(Loss_test_nominal[j], label='nominal')\n",
    "#     plt.plot(Loss_test_mean[j], label='mean')\n",
    "#     plt.plot(np.array(Loss_test_phi[j]), label='phi*a')\n",
    "#     # plt.plot(np.array(Loss_test_exp_forgetting[j]), label='exp forgetting')\n",
    "#     plt.legend()\n",
    "#     plt.title(f'Test data set {j} - {TestData[j].meta[\"condition\"]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose final model\n",
    "stopping_epoch = 199\n",
    "options['num_epochs'] = stopping_epoch\n",
    "final_model = mlmodel.load_model(modelname = modelname + '-epoch-' + str(stopping_epoch))\n",
    "print('Final model loaded:', final_model)\n",
    "print()\n",
    "print(final_model.phi(torch.zeros(11)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the measured aerodynamic force, labeled ground truth (gt), along with the region used for adapation (adapt), and the predicted region (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(Data):\n",
    "    # print('------------------------------')\n",
    "    print(data.meta['condition'] + ':')\n",
    "    mlmodel.vis_validation(t=data.meta['t'], x=data.X, y=data.Y, phi_net=final_model.phi, h_net=final_model.h, idx_adapt_start=0, idx_adapt_end=1000, idx_val_start=1000, idx_val_end=2000, c=Data[i].C, options=options)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in Data:\n",
    "    error_1, error_2, error_3 = mlmodel.error_statistics(data.X, data.Y, final_model.phi, final_model.h, options=options)\n",
    "    print('**** c =', str(data.C), ':', data.meta['condition'], '****')\n",
    "    print(f'Before learning: MSE is {error_1: .2f}')\n",
    "    print(f'Mean predictor: MSE is {error_2: .2f}')\n",
    "    print(f'After learning phi(x): MSE is {error_3: .2f}')\n",
    "    print('')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(TestData):\n",
    "    print('------------------------------')\n",
    "    print(data.meta['condition'] + ':')\n",
    "    mlmodel.vis_validation(t=data.meta['t'], x=data.X, y=data.Y, phi_net=final_model.phi, h_net=final_model.h, idx_adapt_start=0, idx_adapt_end=1000, idx_val_start=1000, idx_val_end=2000, c=Data[i].C, options=options)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in TestData:\n",
    "    error_1, error_2, error_3, t_b, t_m, t_a = mlmodel.error_statistics_with_time(\n",
    "    data.X, data.Y, final_model.phi, final_model.h, options=options)\n",
    "    print('**** c =', str(data.C), ':', data.meta['condition'], '****')\n",
    "    print(f'Before learning:       MSE is {error_1:.2f}   | {t_b:.3f} ms/sample')\n",
    "    print(f'Mean predictor:        MSE is {error_2:.2f}   | {t_m:.3f} ms/sample')\n",
    "    print(f'After learning phi(x): MSE is {error_3:.2f} | {t_a:.3f} ms/sample')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in TestData:\n",
    "    error_1, error_2, error_3 = mlmodel.error_statistics(data.X, data.Y, final_model.phi, final_model.h, options=options)\n",
    "    print('**** :', data.meta['condition'], '****')\n",
    "    print(f'Before learning: MSE is {error_1: .2f}')\n",
    "    print(f'Mean predictor: MSE is {error_2: .2f}')\n",
    "    print(f'After learning phi(x): MSE is {error_3: .2f}')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anylabeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "efbe968048790da5e26fe224257c59db0c3c3cb10bbad9ca12250f2d56e94a61"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
